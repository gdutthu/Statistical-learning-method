@[TOC](统计学习方法 第8章：提升方法)

github链接：[https://github.com/gdutthu/Statistical-learning-method](https://github.com/gdutthu/Statistical-learning-method)
知乎专栏链接：[https://zhuanlan.zhihu.com/c_1252919075576856576](https://zhuanlan.zhihu.com/c_1252919075576856576)





# 1 提出模型




在讲解EM算法的原理之前，我们先通过一个简单的例子来了解EM算法。给大家推荐一个关于EM算法的博客[【机器学习基础】EM算法](https://blog.csdn.net/u010834867/article/details/90762296)，这个博客对EM算法讲的很清楚。在第一章：提出模型，我就是参考的这个博主的博客。





我们有两个硬币A和B，但是两个硬币正反面的概率不一样($P(A)≠P(B)$)。目前我们只有一组观测数据，要求出每一种硬币投掷时正面向上的概率。总共投了五轮，每轮投掷五次，现在先考虑一种简单的情况，假设我们知道这每一轮用的是哪一个硬币去投掷的：



| 硬币 |    结果    |  统计   |
| :--: | :--------: | :-----: |
|  A   | 正正反正反 | 3正-2反 |
|  B   | 反反正正反 | 2正-3反 |
|  A   | 正反反反反 | 1正-4反 |
|  B   | 正反反正正 | 3正-2反 |
|  A   | 反正正反反 | 2正-3反 |

根据上面的数据，就可以得到
$$
P(A)=\frac{3+1+2}{15}=0.4\\
P(B)=\frac{2+3}{10}=0.5\\
$$


现在把问题变得复杂一点，假设我们不知道每一次投掷用的是哪一种硬币，等于是现在的问题加上了一个隐变量，就是每一次选取的硬币的种类。





|  硬币   |    结果    |  统计   |
| :-----: | :--------: | :-----: |
| Unkonwn | 正正反正反 | 3正-2反 |
| Unkonwn | 反反正正反 | 2正-3反 |
| Unkonwn | 正反反反反 | 1正-4反 |
| Unkonwn | 正反反正正 | 3正-2反 |
| Unkonwn | 反正正反反 | 2正-3反 |



假设我们把每一轮实验的硬币的种类设为:$z_{i},i=1,2,3,4,5$,则这五次实验生成了一个5维的向量 $Z=\{z_{1},z_{2},z_{3},z_{4},z_{5}\}$。现在问题来了，

1、如果我们要根据观测结果去求出PA,PB，那么首先需要知道；

2、但是如果用最大似然估计去估计z，又要先求出PA,PB。这就产生了一个循环。

那么这个时候EM算法的作用就体现出来了，EM算法的基本思想是：先初始化一个$P(A),P(B)$，然后我们拿着这个初始化的$P(A),P(B)$用最大似然概率估计出$Z$，接下来有了z之后就用$Z$去计算出在当前$Z$的情况下的$P(A),P(B)$是多少，然后不断地重复这两个步骤直到收敛。




 有了这个思想之后现在用这个思想来做一下这个例子，假设初始状态下$P(A)=0.2, P(B)=0.7$，然后我们根据这个概率去估计出$Z$：



| 轮数 |    若是硬币A，该实验结果的概率     | 实验结果结果 |    若是硬币B，该实验结果的概率    |
| :--: | :--------------------------------: | :----------: | :-------------------------------: |
|  1   |   0.00512，即0.2,0.2,0.2,0.8,0.8   |   3正-2反    | **0.03087**,即0.7,0.7,0.3,0.7,0.3 |
|  2   | **0.02048**，即0.2,0.2,0.8,0.8,0.8 |   2正-3反    |   0.01323,即0.3,0.3,0.7,0.7,0.3   |
|  3   | **0.08192**，即0.2,0.8,0.8,0.8,0.8 |   1正-4反    |   0.00567,即0.7,0.3,0.3,0.3,0.3   |
|  4   |   0.00512，即0.2,0.2,0.2,0.8,0.8   |   3正-2反    | **0.03087**,即0.7,0.3,0.3,0.7,0.7 |
|  5   | **0.02048**，即0.2,0.2,0.8,0.8,0.8 |   2正-3反    |   0.01323,即0.3,0.7,0.7,0.3,0.3   |



按照最大似然估计，$Z=\{B,A,A,B,A\}$，有了z之后我们反过来重新估计一下$P(A),P(B)$：

$$
P(A) =\frac{2+1+2}{15} = 0.33\\
P(B) =\frac{3+3}{10} =0.6\\
$$
可以看到$P(A),P(B)$的值已经更新了，假设$P(A),P(B)$的真实值0.4和0.5，那么你在不断地重复这两步你就会发现$P(A),P(B)$在不断地靠近这两个真实值。



# 2 学习策略


# 3 GMM模型


## 3.2 算法流程



# 4 代码附录

## 4.1 EM算法流程

 最大期望算法经过两个步骤交替进行计算：

**step1:** 计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；

**step2:** 最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。



## 4.2 代码附录



```python
def
```

